# Unicode

## Quick Summary
Unicode, formally known as the Unicode Standard, provides a unique number for every character, regardless of platform, device, application, or language. Modern software providers have adopted it to transport data through these mediums without corruption.

## What is Unicode?
Unicode is a universal character encoding system that assigns a code to the characters and symbols of most languages. It's the only encoding standard that ensures you can retrieve or combine data using an unlimited combination of languages.

Unicode is used in all major operating systems, search engines, browsers, laptops, and smartphones, as well as on the Internet and the World Wide Web (URLs, HTML, XML, CSS, JSON, etc). As of version 13.1, Unicode now encompasses 144,076 characters used in almost every language in the world, alongside emojis. 

To find the Unicode value for emojis, visit the official Unicode website here.

The emergence of Unicode and the availability of tools supporting it are among the most significant recent global software technology trends.

## How does it work?
Unicode defines codes (unique numbers) for all characters used in most languages, including diacritics, punctuation marks, mathematical symbols, technical symbols, arrows, and dingbats. Overall, Unicode has codes for over 100,000 characters from world alphabets, ideograph sets, and symbol collections, including the classical and historical texts of numerous written languages.

Unicode can represent characters in different encoding forms, such as UTF-8 and UTF-16.

UTF-8 is a variable-length encoding scheme in which each written symbol is represented by a one to four-byte code, whereas UTF-16 is a fixed-width encoding scheme in which a two-byte code represents each written symbol.

## ASCII and Unicode
Unicode and ASCII are the most popular character coding standards and are frequently compared as a result. However, they have some essential differences.

Unicode is a universal character encoding method used for the consistent encoding, representation, and handling of text expressed in any spoken language. In comparison, ASCII (American Standard Code for Information Interchange) uses 128 English characters (symbols, letters, digits) as a number to represent text in computers.

A significant difference between these coding methods is size. Unicode represents characters from a more substantial character table than ASCII, which has 128 characters (or 256 with extended ASCII). ASCII represents lowercase letters (a-z), uppercase letters (A-Z), digits (0–9), and symbols such as punctuation marks. In contrast, Unicode represents English, Arabic, Greek, mathematical symbols, historical scripts, emojis, and so on, covering a far greater range of characters than ASCII.

Additionally, the first 128 Unicode characters point to ASCII characters, meaning ASCII can be viewed as a subset of Unicode; this isn't true vice versa.

Unicode also allows characters to be up to 32 bits wide, whereas ASCII uses seven bits (or eight using the 256 version) to represent a character. For Unicode, that’s over four billion unique values to represent all languages accurately. Because Unicode allows for larger bit sizes, it may take up more storage space than ASCII files.

## How and why is it used?
The Unicode encoding system provides a consistent method of encoding multilingual plaintext, making it easier to exchange text files internationally. Supporting data with multiple languages such as French, Japanese, and Hebrew, Unicode allows you to combine different language scripts on a single file. Before Unicode, a computer operating system could only process and display the written symbols on its code page, which was tied to one language script. For example, if a computer could process German, it could not process Hebrew or Japanese.

There is a growing trend for new computer technologies to adopt Unicode. Currently, industry leaders such as Microsoft, Apple, HP, IBM, and Oracle have adopted it. Unicode is also a preferred text encoding method in web browsers Google Chrome and Firefox, and is used in Java technologies HTML, XML, and Windows, and Office. 